"# RNN-Language-Model + SEQ2SEQ model for question answering" 

##### Two main approaches

#### 1- character level 

#### 2- word level

using  LSTM layers to model recurrent neural network to predict next word or char
given the previous ones

#### How To  Use This Code

(will be added soon)

#### TO DO

- [ ] Using state_is_tuple in char level

- [x] Printing number of trainable parameter

- [ ] Training the model

- [ ] Using Estimator

- [X] Converting string data to indexes takes long not to do it every time

- [ ] Code is dirty I've got to clean it

- [x] Saving is not handled in seq2seq

- [x] Plotting loss

- [x] Adding Dropout

- [ ] Last batches are ignored , not getting to batch_size

- [ ] Handle early stopping

